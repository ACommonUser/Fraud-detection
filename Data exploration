1.交易表
包含了交易信息，还包括转账，礼品和服务交易，比如你为其他人订票等。

TransactionDT: 时间戳。第一个值是86400，它对应于一天中的秒数（60 * 60 * 24 = 86400），所以单位是秒。 因为最大值为15811131，我们知道数据跨度为6个月，相当于183天。
TransactionAMT: 美元为单位的交易金额。同时，有的数据有3位小数，并且对应的地址为空，猜测可能是外币交易。
ProductCD: 每条交易的产品编码
card1 - card6: 付款卡的信息，比如卡片种类，发行银行和国家等。
addr: 地址信息。地址的两列均是购买者的地址信息，addr1是地区，addr2是国家。
dist: 某种距离。包括但不限于帐单地址，邮寄地址，邮政编码，IP地址，电话区域等之间的距离。
P_ and (R__) emaildomain: 购买者和收款人的邮箱域名，一些交易不需要收款人，所以R__emaildomain为空。
C1-C14: 加密后的某计数项，例如发现与支付卡关联的地址数等。
D1-D15: t时间戳，例如和前一次交易之间的天数等。
M1-M9: 匹配信息，例如卡上的姓名和地址等。
Vxxx: Vesta生成的丰富的特征，包括排名，计数和其他实体关系。例如，与IP和电子邮件或地址相关联的支付卡在24小时内出现了多少次，等等。所有Vesta这些特征都是以数字形式给出的，里面的一些特征可能是分类特征编码后给出的值。

2.身份表
该表中的字段是加密后的身份信息，比如与交易相关的网络连接信息（IP，ISP，代理等）和数字签名（UA /浏览器/ os /版本等）

其中的分类特征主要有：DeviceType，DeviceInfo，id12 - id38


train['TransactionDT'].plot(kind='hist',
                                        figsize=(15, 5),
                                        label='train',
                                        bins=50,
                                        title='Train vs Test TransactionDT distribution')
test['TransactionDT'].plot(kind='hist',
                                       label='test',
                                       bins=50)
plt.legend()
plt.show()


### 添加hour这个特征 ###
时间是一个很重要的属性，公司给的数据是按秒计算的，需要通过np.floor得到小时，比如49.123小时会转化成49小时，然后取24的余数，就是1。这样49.123小时就变成了1，但是这个1意义不明确，不知道是晚上还是下午。
train["hour"] = np.floor(train["TransactionDT"] / 3600) % 24
test["hour"] = np.floor(train["TransactionDT"] / 3600) % 24

plt.plot(train.groupby('hour').mean()['isFraud'], color='r')
ax = plt.gca()
ax2 = ax.twinx()
_ = ax2.hist(train['hour'], alpha=0.3, bins=24)
ax.set_xlabel('Encoded hour')
ax.set_ylabel('Fraction of fraudulent transactions')

ax2.set_ylabel('Number of transactions')
plt.show()

### ISFRAUD ###
COL = 'isFraud'
trx = tr['isFraud'].value_counts().reset_index().rename({'index':'Value','isFraud':'Count'}, axis=1)
trx['Share'] = np.round(trx['Count'] / trx['Count'].sum(), 6)
display(trx)
corr1(COL)

### TransactionID ###
for col in list(train.columns.values[:COLS_TO_SHOW]):
    proc(col)

### NAN search###
nans_df = train.isna()
nans_groups={}
i_cols = ['V'+str(i) for i in range(1,340)]
for col in train.columns:
    cur_group = nans_df[col].sum()
    try:
        nans_groups[cur_group].append(col)
    except:
        nans_groups[cur_group]=[col]
del nans_df; x=gc.collect()

for k,v in nans_groups.items():
    print('####### NAN count =',k)
    print(v)


####### NAN count = 0
['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']
####### NAN count = 8933
...
####### NAN count = 279287
['D11', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11']
####### NAN count = 525823
...

# V1-11
def make_plots(Vs):
    col = 4
    row = len(Vs)//4+1
    plt.figure(figsize=(20,row*5))
    idx = train[~train[Vs[0]].isna()].index
    for i,v in enumerate(Vs):
        plt.subplot(row,col,i+1)
        n = train[v].nunique()
        x = np.sum(train.loc[idx,v]!=train.loc[idx,v].astype(int))
        y = np.round(100*np.sum(train[v].isna())/len(train),2)
        t = 'int'
        if x!=0: t = 'float'
        plt.title(v+' has '+str(n)+' '+t+' and '+str(y)+'% nan')
        plt.yticks([])
        h = plt.hist(train.loc[idx,v],bins=100)
        if len(h[0])>1: plt.ylim((0,np.sort(h[0])[-2]))
    plt.show()
make_plots(Vs)

def make_corr(Vs,Vtitle=''):
    cols = ['TransactionDT'] + Vs
    plt.figure(figsize=(15,15))
    sns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)
    if Vtitle!='': plt.title(Vtitle,fontsize=14)
    else: plt.title(Vs[0]+' - '+Vs[-1],fontsize=14)
    plt.show()
make_corr(Vs,Vtitle)

grps = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]
def reduce_group(grps,c='V'):
    use = []
    for g in grps:
        mx = 0; vx = g[0]
        for gg in g:
            n = train[c+str(gg)].nunique()
            if n>mx:
                mx = n
                vx = gg
            #print(str(gg)+'-'+str(n),', ',end='')
        use.append(vx)
        #print()
    print('Use these',use)
reduce_group(grps)

cols = ['TransactionDT'] + ['V'+str(x) for x in v]
train2 = train[cols].sample(frac=0.2)
plt.figure(figsize=(15,15))
sns.heatmap(train2[cols].corr(), cmap='RdBu_r', annot=False, center=0.0)
plt.title('V1-V339 REDUCED',fontsize=14)
plt.show()

cols = ['TransactionDT'] + ['V'+str(x) for x in range(1,340)]
train2 = train[cols].sample(frac=0.2)
plt.figure(figsize=(15,15))
sns.heatmap(train2[cols].corr(), cmap='RdBu_r', annot=False, center=0.0)
plt.title('V1-V339 ALL',fontsize=14)
plt.show()
